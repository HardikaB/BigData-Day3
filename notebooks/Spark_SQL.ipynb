{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Spark SQL** - Spark’s interface for working with structured and semistructured data. \n",
    "\n",
    "- **Structured data** is any data that has a schema—that is, a known set of fields for each record.\n",
    "\n",
    "- Spark SQL lets you query structured data inside Spark programs, using either **SQL** or a familiar **DataFrame API**. Usable in Java, Scala, Python and R.\n",
    "\n",
    "- Spark SQL is use to execute SQL queries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In Apache Spark, **a DataFrame is a distributed collection of rows under named columns.**\n",
    "\n",
    "- In simple terms, it is same as a table in relational database or an Excel sheet with Column headers. It also shares some common characteristics with RDD:\n",
    "\n",
    "    - **Immutable in nature :** We can create DataFrame / RDD once but can’t change it. And we can transform a DataFrame / RDD  after applying transformations.\n",
    "    - **Lazy Evaluations:** Which means that a task is not executed until an action is performed.\n",
    "\n",
    "    - **Distributed:** RDD and DataFrame both are distributed in nature.\n",
    "\n",
    "- When running SQL from within another programming language the results will be returned as a DataFrame. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why DataFrames are Useful ?\n",
    "\n",
    "After learning about pandas dataframes, you must be aware of many advantages that Dataframes provides us with. But the question is, what additional advantages Dataframes in spark provides us with?\n",
    "\n",
    "- DataFrames are designed for processing large collection of structured or semi-structured data.\n",
    "\n",
    "- Observations in Spark DataFrame are organised under named columns, which helps Apache Spark to understand the schema of a DataFrame. This helps Spark optimize execution plan on these queries.\n",
    "\n",
    "- DataFrame in Apache Spark has the ability to handle petabytes of data.\n",
    "\n",
    "- DataFrame has a support for wide range of data format and sources.\n",
    "\n",
    "- It has API support for different languages like Python, R, Scala, Java.\n",
    "\n",
    "- Like our RDDs are distibuted across machines in a cluster similarly dataframes provides us with distributed computation capability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As a general rule of thumb, one should consider an alternative to Pandas whenever the data set has more than 10,000,000 rows which, depending on the number of columns and data types, translates to about 5-10 GB of memory usage. At that point PySpark might be an option for you that does the job**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check if SparkContext is running!**\n",
    "\n",
    "**Recall - Why do we need a SparkContext running?**\n",
    "\n",
    "- First step, in any Apache programming is to create a SparkContext. SparkContext is required when we want to execute operations in a cluster. SparkContext tells Spark how and where to access a cluster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.104:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, start a **SQLContext**. Now, Why **SQLContext**?\n",
    "\n",
    "- The entry point into all relational functionality in Spark is the SQLContext class.\n",
    "- Basically it is must to have SQLContext in order to perform SQL related operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Into spark versions<2.0\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Into spark version>2.0\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark dataframe basic example\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: `getOrCreate()`-** Gets an existing SparkSession or, if there is no existing one, creates a new one based on the options set in this builder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to create a DataFrame ?\n",
    "A DataFrame in Apache Spark can be created in multiple ways:\n",
    "\n",
    "It can be created using different data formats. For example:\n",
    "1. Loading data from Existing RDD.\n",
    "2. Loading the data from JSON, CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  Creating DataFrame from RDD\n",
    "\n",
    "One can easily create a dataframe out of a List of tuples. Steps can be as follows:\n",
    "\n",
    "1. Create a list of tuples. Each tuple contains name of a person with age.\n",
    "2. Create a RDD from the list above.\n",
    "3. Convert each tuple to a row.\n",
    "4. Create a DataFrame by applying createDataFrame on RDD with the help of sqlContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "l = [('Sam',25, 'M'),('Jalfaizy',22, 'F'),('Tom',20, 'M'),('Nicky',26, 'F'),('Wrick', 30, 'M')]\n",
    "rdd = sc.parallelize(l)\n",
    "people = rdd.map(lambda x: Row(name=x[0], age=int(x[1]), Gender=x[2]))\n",
    "schemaPeople = sqlContext.createDataFrame(people)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the type!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(schemaPeople)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating the DataFrame from external file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Dataset\n",
    "\n",
    "### Context\n",
    "\n",
    "H-1B visas are a category of employment-based, non-immigrant visas for temporary foreign workers in the United States. For a foreign national to apply for H1-B visa, a US employer must offer them a job and submit a petition for a H-1B visa to the US immigration department. This is also the most common visa status applied for and held by international students once they complete college or higher education and begin working in a full-time position.\n",
    "\n",
    "This dataset contains H-1B petition data. The columns in the dataset include case status, employer name, worksite coordinates, job title, prevailing wage, occupation code, and year filed.\n",
    "\n",
    "For more information on individual columns, refer to the column metadata. A detailed description of the underlying raw dataset is available in an [official data dictionary](https://www.foreignlaborcert.doleta.gov/docs/Performance_Data/Disclosure/FY15-FY16/H-1B_FY16_Record_Layout.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my tutorial session, I'll use the data file `h1b_sample.csv` but would strongly recommed that you use the data file `h1b_learners.csv` to do the hands-on practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('../data/h1b_sample.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we read data into the SQLContext object, Spark:\n",
    "\n",
    "- Instantiates a Spark DataFrame object\n",
    "- Infers the schema from the data and associates it with the DataFrame\n",
    "- Reads in the data and distributes it across clusters (if multiple clusters are available)\n",
    "- Returns the DataFrame object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the **schema for the DataFrame** we created out of our dataset. For this, we can call `printSchema()` method on our dataframe. This will provide us the datatype of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CASE_STATUS: string (nullable = true)\n",
      " |-- EMPLOYER_NAME: string (nullable = true)\n",
      " |-- SOC_NAME: string (nullable = true)\n",
      " |-- JOB_TITLE: string (nullable = true)\n",
      " |-- FULL_TIME_POSITION: string (nullable = true)\n",
      " |-- PREVAILING_WAGE: string (nullable = true)\n",
      " |-- YEAR: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`show()`** method on a DataFrame can give us a quick look on rows of the DataFame. Use `show()` to display 5 Rows of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+--------------------+------------------+---------------+----+\n",
      "|        CASE_STATUS|       EMPLOYER_NAME|            SOC_NAME|           JOB_TITLE|FULL_TIME_POSITION|PREVAILING_WAGE|YEAR|\n",
      "+-------------------+--------------------+--------------------+--------------------+------------------+---------------+----+\n",
      "|CERTIFIED-WITHDRAWN|UNIVERSITY OF MIC...|BIOCHEMISTS AND B...|POSTDOCTORAL RESE...|                 N|        36067.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|GOODMAN NETWORKS,...|    CHIEF EXECUTIVES|CHIEF OPERATING O...|                 Y|       242674.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|PORTS AMERICA GRO...|    CHIEF EXECUTIVES|CHIEF PROCESS OFF...|                 Y|       193066.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|GATES CORPORATION...|    CHIEF EXECUTIVES|REGIONAL PRESIDEN...|                 Y|       220314.0|2016|\n",
      "|          WITHDRAWN|PEABODY INVESTMEN...|    CHIEF EXECUTIVES|PRESIDENT MONGOLI...|                 Y|       157518.4|2016|\n",
      "|CERTIFIED-WITHDRAWN|BURGER KING CORPO...|    CHIEF EXECUTIVES|EXECUTIVE V P, GL...|                 Y|       225000.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|BT AND MK ENERGY ...|    CHIEF EXECUTIVES|CHIEF OPERATING O...|                 Y|        91021.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|GLOBO MOBILE TECH...|    CHIEF EXECUTIVES|CHIEF OPERATIONS ...|                 Y|       150000.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|  ESI COMPANIES INC.|    CHIEF EXECUTIVES|           PRESIDENT|                 Y|       127546.0|2016|\n",
      "|          WITHDRAWN|LESSARD INTERNATI...|    CHIEF EXECUTIVES|           PRESIDENT|                 Y|       154648.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|  H.J. HEINZ COMPANY|    CHIEF EXECUTIVES|CHIEF INFORMATION...|                 Y|       182978.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|DOW CORNING CORPO...|    CHIEF EXECUTIVES|VICE PRESIDENT AN...|                 Y|       163717.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|    ACUSHNET COMPANY|    CHIEF EXECUTIVES|   TREASURER AND COO|                 Y|       203860.8|2016|\n",
      "|CERTIFIED-WITHDRAWN|       BIOCAIR, INC.|    CHIEF EXECUTIVES|CHIEF COMMERCIAL ...|                 Y|       252637.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|NEWMONT MINING CO...|    CHIEF EXECUTIVES|        BOARD MEMBER|                 Y|       105914.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|        VRICON, INC.|    CHIEF EXECUTIVES|CHIEF FINANCIAL O...|                 Y|       153046.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|CARDIAC SCIENCE C...|  FINANCIAL MANAGERS|VICE PRESIDENT OF...|                 Y|        90834.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|WESTFIELD CORPORA...|    CHIEF EXECUTIVES|GENERAL MANAGER, ...|                 Y|       164050.0|2016|\n",
      "|          CERTIFIED|      QUICKLOGIX LLC|    CHIEF EXECUTIVES|                 CEO|                 Y|       187200.0|2016|\n",
      "|          CERTIFIED|MCCHRYSTAL GROUP,...|    CHIEF EXECUTIVES|PRESIDENT, NORTHE...|                 Y|       241842.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|   CUDDLE BARN, INC.|    CHIEF EXECUTIVES|CHIEF OPERATING O...|                 Y|       117998.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|WESTFIELD CORPORA...|    CHIEF EXECUTIVES|GENERAL MANAGER, ...|                 Y|       164050.0|2016|\n",
      "|          CERTIFIED|         LOMICS, LLC|    CHIEF EXECUTIVES|                 CEO|                 Y|        99986.0|2016|\n",
      "|          CERTIFIED|UC UNIVERSITY HIG...|    CHIEF EXECUTIVES|CHIEF FINANCIAL O...|                 Y|        99986.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|VMS COMMUNICATION...|    CHIEF EXECUTIVES|CHIEF OPERATING O...|                 Y|       159370.0|2016|\n",
      "|          CERTIFIED|    QUICKLOGIX, INC.|    CHIEF EXECUTIVES|                 CEO|                 Y|       187200.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|FOODESSENTIALS CO...|    CHIEF EXECUTIVES|CHIEF EXECUTIVE O...|                 Y|       130853.0|2016|\n",
      "|          CERTIFIED|          HELLO INC.|    CHIEF EXECUTIVES|CHIEF BUSINESS OF...|                 Y|       215862.0|2016|\n",
      "|          CERTIFIED|          UMBEL CORP|    CHIEF EXECUTIVES|VICE PRESIDENT OF...|                 Y|       192088.0|2016|\n",
      "|          CERTIFIED|PERSPECTIVES OF F...|    CHIEF EXECUTIVES|  EXECUTIVE DIRECTOR|                 Y|       95295.98|2016|\n",
      "|          CERTIFIED|            GTH INC.|    CHIEF EXECUTIVES|VICE PRESIDENT, B...|                 Y|       149594.0|2016|\n",
      "|          CERTIFIED|THE KRAFT HEINZ C...|    CHIEF EXECUTIVES|    HEAD OF US SALES|                 Y|       226699.0|2016|\n",
      "|          CERTIFIED|         REGED, INC.|    CHIEF EXECUTIVES|CHIEF FINANCIAL O...|                 Y|       187200.0|2016|\n",
      "|          CERTIFIED|VMS COMMUNICATION...|    CHIEF EXECUTIVES|CHIEF OPERATING O...|                 Y|       159370.0|2016|\n",
      "|          CERTIFIED|FINLAY EXTRACTS &...|    CHIEF EXECUTIVES|VICE PRESIDENT OF...|                 Y|        98550.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|       LABEL INSIGHT|    CHIEF EXECUTIVES|CHIEF EXECUTIVE O...|                 Y|       130853.0|2016|\n",
      "|          CERTIFIED|       LABEL INSIGHT|    CHIEF EXECUTIVES|CHIEF EXECUTIVE O...|                 Y|       130853.0|2016|\n",
      "|          CERTIFIED|INN AT THE WICKLI...|    CHIEF EXECUTIVES|CHIEF EXECUTIVE O...|                 N|        52416.0|2016|\n",
      "|          CERTIFIED|DURHAM SCHOOL SER...|    CHIEF EXECUTIVES|VICE PRESIDENT, F...|                 Y|       130853.0|2016|\n",
      "|             DENIED|     PARALLELS, INC.|    CHIEF EXECUTIVES|CHIEF EXECUTIVE O...|                 Y|       187200.0|2016|\n",
      "|          CERTIFIED|  CB LANSING 300 LLP|    CHIEF EXECUTIVES|CHIEF OPERATING O...|                 Y|        89107.0|2016|\n",
      "|          CERTIFIED|SOFTTEK INTEGRATI...|    CHIEF EXECUTIVES|    ACCOUNT DIRECTOR|                 Y|       130853.0|2016|\n",
      "|          CERTIFIED|       SPARITY, INC.|    CHIEF EXECUTIVES|  TECHNICAL DIRECTOR|                 Y|       102190.0|2016|\n",
      "|          CERTIFIED|ADCONION DIRECT, ...|    CHIEF EXECUTIVES|VICE PRESIDENT OF...|                 Y|       197683.0|2016|\n",
      "|          CERTIFIED|   MAVENCARE US INC.|    CHIEF EXECUTIVES|SVP BUSINESS OPER...|                 Y|       122179.0|2016|\n",
      "|          CERTIFIED|THE CHICAGO ATHEN...|    CHIEF EXECUTIVES|  EXECUTIVE DIRECTOR|                 N|        54766.0|2016|\n",
      "|          CERTIFIED|   MED ADVANTAGE LLC|    CHIEF EXECUTIVES|CHIEF FINANCIAL O...|                 Y|       187200.0|2016|\n",
      "|             DENIED|RANCHO LA PUERTA LLC|    CHIEF EXECUTIVES|           PRESIDENT|                 Y|       197683.2|2016|\n",
      "|          CERTIFIED|    STRATEGISM, INC.|    CHIEF EXECUTIVES|CHIEF EXECUTIVE O...|                 Y|       155459.2|2016|\n",
      "|          CERTIFIED|RANCHO LA PUERTA LLC|    CHIEF EXECUTIVES|           PRESIDENT|                 Y|       197683.2|2016|\n",
      "+-------------------+--------------------+--------------------+--------------------+------------------+---------------+----+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics\n",
    "\n",
    "Let's have statistical view of our dataframe.\n",
    "\n",
    "We can use `describe(*cols)` method on a dataframe to compute statistics for numeric and string columns.\n",
    "\n",
    "This include count, mean, stddev, min, and max. If no columns are given, this function computes statistics for all numerical or string columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------------------+--------------------+--------------------+------------------+------------------+------+\n",
      "|summary|CASE_STATUS|       EMPLOYER_NAME|            SOC_NAME|           JOB_TITLE|FULL_TIME_POSITION|   PREVAILING_WAGE|  YEAR|\n",
      "+-------+-----------+--------------------+--------------------+--------------------+------------------+------------------+------+\n",
      "|  count|       5000|                5000|                5000|                5000|              5000|              5000|  5000|\n",
      "|   mean|       null|                null|                null|                null|              null|127988.90342799998|2016.0|\n",
      "| stddev|       null|                null|                null|                null|              null|1029719.7954029571|   0.0|\n",
      "|    min|  CERTIFIED|+421 FOUNDATION INC.|ADVERTISING AND P...|             11-1021|                 N|          100000.0|  2016|\n",
      "|    max|  WITHDRAWN|ZYME SOLUTIONS, INC.|PUBLIC RELATIONS ...|WW MARKETING MANA...|                 Y|           99986.0|  2016|\n",
      "+-------+-----------+--------------------+--------------------+--------------------+------------------+------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pandas, we used the `head()` method to return the first n rows. This is one of the differences between the DataFrame implementations. Instead of returning a nicely formatted table of values, the head() method in Spark returns a list of row objects. Spark needs to return row objects for certain methods, such as head(), collect() and take().\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(CASE_STATUS=u'CERTIFIED-WITHDRAWN', EMPLOYER_NAME=u'UNIVERSITY OF MICHIGAN', SOC_NAME=u'BIOCHEMISTS AND BIOPHYSICISTS', JOB_TITLE=u'POSTDOCTORAL RESEARCH FELLOW', FULL_TIME_POSITION=u'N', PREVAILING_WAGE=u'36067.0', YEAR=u'2016'),\n",
       " Row(CASE_STATUS=u'CERTIFIED-WITHDRAWN', EMPLOYER_NAME=u'GOODMAN NETWORKS, INC.', SOC_NAME=u'CHIEF EXECUTIVES', JOB_TITLE=u'CHIEF OPERATING OFFICER', FULL_TIME_POSITION=u'Y', PREVAILING_WAGE=u'242674.0', YEAR=u'2016'),\n",
       " Row(CASE_STATUS=u'CERTIFIED-WITHDRAWN', EMPLOYER_NAME=u'PORTS AMERICA GROUP, INC.', SOC_NAME=u'CHIEF EXECUTIVES', JOB_TITLE=u'CHIEF PROCESS OFFICER', FULL_TIME_POSITION=u'Y', PREVAILING_WAGE=u'193066.0', YEAR=u'2016'),\n",
       " Row(CASE_STATUS=u'CERTIFIED-WITHDRAWN', EMPLOYER_NAME=u'GATES CORPORATION, A WHOLLY-OWNED SUBSIDIARY OF TOMKINS PLC', SOC_NAME=u'CHIEF EXECUTIVES', JOB_TITLE=u'REGIONAL PRESIDEN, AMERICAS', FULL_TIME_POSITION=u'Y', PREVAILING_WAGE=u'220314.0', YEAR=u'2016'),\n",
       " Row(CASE_STATUS=u'WITHDRAWN', EMPLOYER_NAME=u'PEABODY INVESTMENTS CORP.', SOC_NAME=u'CHIEF EXECUTIVES', JOB_TITLE=u'PRESIDENT MONGOLIA AND INDIA', FULL_TIME_POSITION=u'Y', PREVAILING_WAGE=u'157518.4', YEAR=u'2016')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next, print the first row out the five fetched rows. Then print the `EMPLOYER_NAME` for the first row entry.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(CASE_STATUS=u'CERTIFIED-WITHDRAWN', EMPLOYER_NAME=u'UNIVERSITY OF MICHIGAN', SOC_NAME=u'BIOCHEMISTS AND BIOPHYSICISTS', JOB_TITLE=u'POSTDOCTORAL RESEARCH FELLOW', FULL_TIME_POSITION=u'N', PREVAILING_WAGE=u'36067.0', YEAR=u'2016')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'UNIVERSITY OF MICHIGAN'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)[0].EMPLOYER_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSTDOCTORAL RESEARCH FELLOW\n",
      "CHIEF OPERATING OFFICER\n",
      "CHIEF PROCESS OFFICER\n",
      "REGIONAL PRESIDEN, AMERICAS\n",
      "PRESIDENT MONGOLIA AND INDIA\n"
     ]
    }
   ],
   "source": [
    "first_five = df.head(5)\n",
    "for each_element in first_five:\n",
    "    print each_element.JOB_TITLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting columns\n",
    "In pandas, we pass a string into a single pair of brackets ([]) to select an individual column, and pass in a list to select multiple columns. For example:\n",
    "\n",
    "#### Pandas DataFrame\n",
    "df['age']\n",
    "\n",
    "df[['age', 'males']]\n",
    "\n",
    "Spark also allows us to use bracket notation. Pass in a list of string objects with column name to select any column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print the age value for first five employees in the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[YEAR: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('YEAR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframes being lazily evaluated like RDDs will only display the results of an operation when we call any action upon it. We can call the show() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|YEAR|\n",
      "+----+\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('YEAR').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display Employer Name with their case status.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: Use select() to display required columns\n",
    "# df.select()\n",
    "# df[[]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the total number of rows in our dataframe. We can use count() to give us total number of rows in our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop Rows containing NULL values**\n",
    "\n",
    "We can use `drop(how='any', thresh=None, subset=None)` method on our dataframe to drop rows with null values and return a new dataframe.\n",
    "\n",
    "**Parameters:**\t\n",
    "\n",
    "**how** – ‘any’ or ‘all’. If ‘any’, drop a row if it contains any nulls. If ‘all’, drop a row only if all its values are null.\n",
    "\n",
    "**thresh** – int, default None If specified, drop rows that have less than thresh non-null values. This overwrites the how parameter.\n",
    "\n",
    "**subset** – optional list of column names to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+--------------------+------------------+---------------+----+\n",
      "|        CASE_STATUS|       EMPLOYER_NAME|            SOC_NAME|           JOB_TITLE|FULL_TIME_POSITION|PREVAILING_WAGE|YEAR|\n",
      "+-------------------+--------------------+--------------------+--------------------+------------------+---------------+----+\n",
      "|CERTIFIED-WITHDRAWN|UNIVERSITY OF MIC...|BIOCHEMISTS AND B...|POSTDOCTORAL RESE...|                 N|        36067.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|GOODMAN NETWORKS,...|    CHIEF EXECUTIVES|CHIEF OPERATING O...|                 Y|       242674.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|PORTS AMERICA GRO...|    CHIEF EXECUTIVES|CHIEF PROCESS OFF...|                 Y|       193066.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|GATES CORPORATION...|    CHIEF EXECUTIVES|REGIONAL PRESIDEN...|                 Y|       220314.0|2016|\n",
      "|          WITHDRAWN|PEABODY INVESTMEN...|    CHIEF EXECUTIVES|PRESIDENT MONGOLI...|                 Y|       157518.4|2016|\n",
      "|CERTIFIED-WITHDRAWN|BURGER KING CORPO...|    CHIEF EXECUTIVES|EXECUTIVE V P, GL...|                 Y|       225000.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|BT AND MK ENERGY ...|    CHIEF EXECUTIVES|CHIEF OPERATING O...|                 Y|        91021.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|GLOBO MOBILE TECH...|    CHIEF EXECUTIVES|CHIEF OPERATIONS ...|                 Y|       150000.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|  ESI COMPANIES INC.|    CHIEF EXECUTIVES|           PRESIDENT|                 Y|       127546.0|2016|\n",
      "|          WITHDRAWN|LESSARD INTERNATI...|    CHIEF EXECUTIVES|           PRESIDENT|                 Y|       154648.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|  H.J. HEINZ COMPANY|    CHIEF EXECUTIVES|CHIEF INFORMATION...|                 Y|       182978.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|DOW CORNING CORPO...|    CHIEF EXECUTIVES|VICE PRESIDENT AN...|                 Y|       163717.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|    ACUSHNET COMPANY|    CHIEF EXECUTIVES|   TREASURER AND COO|                 Y|       203860.8|2016|\n",
      "|CERTIFIED-WITHDRAWN|       BIOCAIR, INC.|    CHIEF EXECUTIVES|CHIEF COMMERCIAL ...|                 Y|       252637.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|NEWMONT MINING CO...|    CHIEF EXECUTIVES|        BOARD MEMBER|                 Y|       105914.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|        VRICON, INC.|    CHIEF EXECUTIVES|CHIEF FINANCIAL O...|                 Y|       153046.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|CARDIAC SCIENCE C...|  FINANCIAL MANAGERS|VICE PRESIDENT OF...|                 Y|        90834.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|WESTFIELD CORPORA...|    CHIEF EXECUTIVES|GENERAL MANAGER, ...|                 Y|       164050.0|2016|\n",
      "|          CERTIFIED|      QUICKLOGIX LLC|    CHIEF EXECUTIVES|                 CEO|                 Y|       187200.0|2016|\n",
      "|          CERTIFIED|MCCHRYSTAL GROUP,...|    CHIEF EXECUTIVES|PRESIDENT, NORTHE...|                 Y|       241842.0|2016|\n",
      "+-------------------+--------------------+--------------------+--------------------+------------------+---------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.na.drop()\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace Null values\n",
    "**What if don't want to drop entire row but just replace the null values?**\n",
    "\n",
    "`fillna(value, subset=None)` enables us to replace null values in our dataframe. We can optionally specify the set of columns into which we want to replace nul values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace null values in all the columns\n",
    "df = df.fillna(0)\n",
    "# df.fillna(0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace null values only in the Columns `CASE_STATUS` and `EMPLOYER_NAME`. \n",
    "\n",
    "Hint: Use `fillna(value, subset=None)` and specify required column names in the subset parameter. For example - `df.fillna(0, subset=['a', 'b'])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_filterd = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the possible categories in the case status?\n",
    "\n",
    "`distinct()`: Returns a new DataFrame containing the distinct rows in this DataFrame.\n",
    "\n",
    "So next, select the `CASE_STATUS` column and apply `distint()` method on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|        CASE_STATUS|\n",
      "+-------------------+\n",
      "|          CERTIFIED|\n",
      "|CERTIFIED-WITHDRAWN|\n",
      "|          WITHDRAWN|\n",
      "|             DENIED|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('CASE_STATUS').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine Distinct `CASE_STATUS` count for each `EMPLOYER_NAME`\n",
    "\n",
    "For example, determine how many visa applications are certified under the employer name `SAMSUNG ELECTRONICS`\n",
    "\n",
    "We can use `crosstab()` method to get this done. **crosstab(col1, col2)** computes a pair-wise frequency table of the given columns.\n",
    "\n",
    "**Parameters:**\t\n",
    "\n",
    "**col1** – The name of the first column. Distinct items will make the first item of each row.\n",
    "\n",
    "**col2** – The name of the second column. Distinct items will make the column names of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.crosstab('EMPLOYER_NAME', 'CASE_STATUS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Determine the top Employers getting more visa applications into a Certified Status**\n",
    "\n",
    "Find out the top 10 companies having highest number of certified visa applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+---------+-------------------+------+---------+\n",
      "|EMPLOYER_NAME_CASE_STATUS|CERTIFIED|CERTIFIED-WITHDRAWN|DENIED|WITHDRAWN|\n",
      "+-------------------------+---------+-------------------+------+---------+\n",
      "|        PROFORM GROUP INC|        2|                  0|     0|        0|\n",
      "|          MIROCULUS, INC.|        1|                  0|     0|        0|\n",
      "|       TTI LEXINGTON, LLC|        2|                  0|     0|        0|\n",
      "|     SAMSUNG ELECTRONI...|        4|                  1|     0|        1|\n",
      "|         SWAGELOK COMPANY|        1|                  0|     0|        0|\n",
      "|     MOSS & ASSOCIATES...|        1|                  0|     0|        1|\n",
      "|            QULINARY INC.|        1|                  0|     0|        0|\n",
      "|       IDM PRODUCTION LLC|        0|                  1|     0|        0|\n",
      "|              ACCUEN INC.|        2|                  1|     0|        0|\n",
      "|          COVER-MORE INC.|        1|                  0|     1|        0|\n",
      "|     AMENITY SERVICES,...|        1|                  0|     0|        0|\n",
      "|          KEEN LABS, INC.|        1|                  0|     0|        0|\n",
      "|        RINA SYSTEMS, LLC|        1|                  0|     0|        0|\n",
      "|           QUICKLOGIX LLC|        1|                  0|     0|        0|\n",
      "|     INTERACTIVE LIFE ...|        1|                  0|     0|        0|\n",
      "|     EVER CURIOUS CORP...|        1|                  0|     0|        1|\n",
      "|             ELBRUS, INC.|        1|                  0|     0|        0|\n",
      "|     LEO BURNETT COMPA...|        1|                  0|     0|        0|\n",
      "|       COEUR MINING, INC.|        1|                  1|     0|        0|\n",
      "|           ETOUCHES, INC.|        1|                  0|     0|        0|\n",
      "+-------------------------+---------+-------------------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+---------+-------------------+------+---------+\n",
      "|EMPLOYER_NAME_CASE_STATUS|CERTIFIED|CERTIFIED-WITHDRAWN|DENIED|WITHDRAWN|\n",
      "+-------------------------+---------+-------------------+------+---------+\n",
      "|     AMAZON CORPORATE LLC|       50|                  1|     0|        1|\n",
      "|     ADOBE SYSTEMS INC...|       32|                  0|     1|        3|\n",
      "|               APPLE INC.|       20|                  0|     0|        0|\n",
      "|      CISCO SYSTEMS, INC.|       16|                  0|     1|        0|\n",
      "|              GOOGLE INC.|       15|                  6|     0|        0|\n",
      "|     WAL-MART ASSOCIAT...|       14|                  2|     0|        3|\n",
      "|             VMWARE, INC.|       12|                  0|     0|        0|\n",
      "|          EMC CORPORATION|       11|                  0|     0|        0|\n",
      "|     SEARS HOLDINGS MA...|        9|                  9|     0|        0|\n",
      "|     BECTON, DICKINSON...|        9|                  0|     0|        0|\n",
      "|     BURGER KING CORPO...|        8|                  1|     0|        0|\n",
      "|      GENENTECH USA, INC.|        8|                  1|     0|        2|\n",
      "|          ECOLAB USA INC.|        8|                  6|     0|        0|\n",
      "|            NETFLIX, INC.|        8|                  0|     0|        0|\n",
      "|     MICROSOFT CORPORA...|        8|                  4|     0|        0|\n",
      "|              DROGA5, LLC|        8|                  0|     0|        0|\n",
      "|        EPAM SYSTEMS, INC|        8|                  0|     0|        2|\n",
      "|       CVS PHARMACY, INC.|        8|                  3|     0|        1|\n",
      "|               ABBVIE INC|        8|                  0|     0|        0|\n",
      "|      GENERAL MILLS, INC.|        7|                  0|     0|        0|\n",
      "+-------------------------+---------+-------------------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df2.orderBy(df2['CERTIFIED'].desc())\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which `JOB_TITLE` got the highest number of certified visa applications?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.crosstab('JOB_TITLE', 'CASE_STATUS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------+-------------------+------+---------+\n",
      "|JOB_TITLE_CASE_STATUS|CERTIFIED|CERTIFIED-WITHDRAWN|DENIED|WITHDRAWN|\n",
      "+---------------------+---------+-------------------+------+---------+\n",
      "|   OPERATIONS MANAGER|      206|                  9|    21|        6|\n",
      "|    MARKETING MANAGER|      191|                 11|    25|        9|\n",
      "|      GENERAL MANAGER|      128|                  2|    17|        3|\n",
      "| CHIEF EXECUTIVE O...|      127|                  3|    17|        5|\n",
      "|      PRODUCT MANAGER|       94|                 13|     0|        7|\n",
      "| BUSINESS DEVELOPM...|       91|                  2|     1|        2|\n",
      "| CHIEF OPERATING O...|       70|                  9|     6|        4|\n",
      "| PRODUCT MARKETING...|       47|                  8|     0|        4|\n",
      "| DIRECTOR OF OPERA...|       46|                  0|     3|        2|\n",
      "| SENIOR PRODUCT MA...|       45|                  5|     2|        0|\n",
      "|    MANAGING DIRECTOR|       34|                  2|     3|        0|\n",
      "|            PRESIDENT|       33|                  2|     2|        1|\n",
      "|                  CEO|       31|                  0|     5|        1|\n",
      "| GENERAL AND OPERA...|       29|                  1|     7|        4|\n",
      "|     ACCOUNT DIRECTOR|       26|                  1|     2|        2|\n",
      "|   EXECUTIVE DIRECTOR|       24|                  0|     1|        0|\n",
      "| BUSINESS DEVELOPM...|       23|                  0|     3|        0|\n",
      "| ADVERTISING AND P...|       23|                  0|     4|        1|\n",
      "|      PROJECT MANAGER|       21|                  1|     2|        1|\n",
      "|       VICE PRESIDENT|       19|                  2|     1|        0|\n",
      "+---------------------+---------+-------------------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4 = df3.orderBy(df3['CERTIFIED'].desc())\n",
    "df3.orderBy(df3['CERTIFIED'].desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations on columns\n",
    "\n",
    "**Classify Application status for each job title as either `Certified` or `NON-CERTIFIED`.**\n",
    "\n",
    "Hint: For each row we can sum up the values of columns `CERTIFIED-WITHDRAWN` + `WITHDRAWN` + `DENIED` into one single column as `NON-CERTIFIED`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------+--------------------------------------------+\n",
      "|JOB_TITLE_CASE_STATUS|CERTIFIED|((CERTIFIED-WITHDRAWN + WITHDRAWN) + DENIED)|\n",
      "+---------------------+---------+--------------------------------------------+\n",
      "|   OPERATIONS MANAGER|      206|                                          36|\n",
      "|    MARKETING MANAGER|      191|                                          45|\n",
      "|      GENERAL MANAGER|      128|                                          22|\n",
      "| CHIEF EXECUTIVE O...|      127|                                          25|\n",
      "|      PRODUCT MANAGER|       94|                                          20|\n",
      "| BUSINESS DEVELOPM...|       91|                                           5|\n",
      "| CHIEF OPERATING O...|       70|                                          19|\n",
      "| PRODUCT MARKETING...|       47|                                          12|\n",
      "| DIRECTOR OF OPERA...|       46|                                           5|\n",
      "| SENIOR PRODUCT MA...|       45|                                           7|\n",
      "|    MANAGING DIRECTOR|       34|                                           5|\n",
      "|            PRESIDENT|       33|                                           5|\n",
      "|                  CEO|       31|                                           6|\n",
      "| GENERAL AND OPERA...|       29|                                          12|\n",
      "|     ACCOUNT DIRECTOR|       26|                                           5|\n",
      "|   EXECUTIVE DIRECTOR|       24|                                           1|\n",
      "| ADVERTISING AND P...|       23|                                           5|\n",
      "| BUSINESS DEVELOPM...|       23|                                           3|\n",
      "|      PROJECT MANAGER|       21|                                           4|\n",
      "|        BRAND MANAGER|       19|                                           0|\n",
      "+---------------------+---------+--------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5 = df4.select(df4['JOB_TITLE_CASE_STATUS'], df4['CERTIFIED'], df4['CERTIFIED-WITHDRAWN']+df4['WITHDRAWN']+df4['DENIED'])\n",
    "\n",
    "df4.select(df4['JOB_TITLE_CASE_STATUS'], df4['CERTIFIED'], df4['CERTIFIED-WITHDRAWN']+df4['WITHDRAWN']+df4['DENIED']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = df5.select('JOB_TITLE_CASE_STATUS', 'CERTIFIED', col(\"((CERTIFIED-WITHDRAWN + WITHDRAWN) + DENIED)\").alias(\"NON-CERTIFIED\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------+-------------+\n",
      "|JOB_TITLE_CASE_STATUS|CERTIFIED|NON-CERTIFIED|\n",
      "+---------------------+---------+-------------+\n",
      "|   OPERATIONS MANAGER|      206|           36|\n",
      "|    MARKETING MANAGER|      191|           45|\n",
      "|      GENERAL MANAGER|      128|           22|\n",
      "| CHIEF EXECUTIVE O...|      127|           25|\n",
      "|      PRODUCT MANAGER|       94|           20|\n",
      "| BUSINESS DEVELOPM...|       91|            5|\n",
      "| CHIEF OPERATING O...|       70|           19|\n",
      "| PRODUCT MARKETING...|       47|           12|\n",
      "| DIRECTOR OF OPERA...|       46|            5|\n",
      "| SENIOR PRODUCT MA...|       45|            7|\n",
      "|    MANAGING DIRECTOR|       34|            5|\n",
      "|            PRESIDENT|       33|            5|\n",
      "|                  CEO|       31|            6|\n",
      "| GENERAL AND OPERA...|       29|           12|\n",
      "|     ACCOUNT DIRECTOR|       26|            5|\n",
      "|   EXECUTIVE DIRECTOR|       24|            1|\n",
      "| ADVERTISING AND P...|       23|            5|\n",
      "| BUSINESS DEVELOPM...|       23|            3|\n",
      "|      PROJECT MANAGER|       21|            4|\n",
      "|       VICE PRESIDENT|       19|            3|\n",
      "+---------------------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find the total number of `CERTIFIED` and `NON-CERTIFIED` applications in your dataframe.**\n",
    "\n",
    "Hint: Use aggregation function like sum() to compute total number in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4149.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_certified = float(df6.groupBy().sum('CERTIFIED').collect()[0][0])\n",
    "total_certified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly, calculate total number of NON-CERTIFIED applications.\n",
    "\n",
    "# total_noncertified = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s the problem: I have a Python function that iterates over my data, but going through each row in the dataframe takes several days. If I have a computing cluster with many nodes, how can I distribute this Python function in PySpark to speed up this process — maybe cut the total time down to less than a few hours — with the least amount of work?\n",
    "\n",
    "In other words, how do I turn a Python function into a Spark user defined function, or UDF?\n",
    "\n",
    "<img src = \"../images/dataframe.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can we make use of User Defined Functions on our Dataframes?\n",
    "\n",
    "Recall, what was the use of `map()` and `flatMap()` methods when we were operating on our RDDs. Basically these help us to apply the user defined functions on each partition of our RDD.\n",
    "\n",
    "Similarly, spark allow us to operate on dataframe using our custom functions.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Define your custom function\n",
    "2. Register UDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def share(s):\n",
    "  return (s / total_certified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering a UDF\n",
    "\n",
    "- PySpark UDFs work in a similar way as the pandas .map() and .apply() methods for pandas series and dataframes. If I have a function that can use values from a row in the dataframe as input, then I can map it to the entire dataframe. The only difference is that with PySpark UDFs we have to specify the output data type.\n",
    "\n",
    "- As long as the python function’s output has a corresponding data type in Spark, it can be turned into a UDF. When registering UDFs, we have to specify the data type using the types from pyspark.sql.types. All the types supported by PySpark [can be found here](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=types#module-pyspark.sql.types).\n",
    "\n",
    "- udf(): Returns a **UDFRegistration** for UDF registration.\n",
    "\n",
    "- register(name, f, returnType=StringType): Registers a python function (including lambda function) as a **UDF** so it can be used in SQL statements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4149.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "sqlContext.udf.register(\"Employershare\", share)\n",
    "\n",
    "print total_certified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+---------+\n",
      "|EMPLOYER_NAME_CASE_STATUS|CERTIFIED|\n",
      "+-------------------------+---------+\n",
      "|     AMAZON CORPORATE LLC|     50.0|\n",
      "|     ADOBE SYSTEMS INC...|     32.0|\n",
      "|               APPLE INC.|     20.0|\n",
      "|      CISCO SYSTEMS, INC.|     16.0|\n",
      "|              GOOGLE INC.|     15.0|\n",
      "|     WAL-MART ASSOCIAT...|     14.0|\n",
      "|             VMWARE, INC.|     12.0|\n",
      "|          EMC CORPORATION|     11.0|\n",
      "|     SEARS HOLDINGS MA...|      9.0|\n",
      "|     BECTON, DICKINSON...|      9.0|\n",
      "|          ECOLAB USA INC.|      8.0|\n",
      "|      GENENTECH USA, INC.|      8.0|\n",
      "|        EPAM SYSTEMS, INC|      8.0|\n",
      "|     MICROSOFT CORPORA...|      8.0|\n",
      "|               ABBVIE INC|      8.0|\n",
      "|              DROGA5, LLC|      8.0|\n",
      "|            NETFLIX, INC.|      8.0|\n",
      "|       CVS PHARMACY, INC.|      8.0|\n",
      "|     BURGER KING CORPO...|      8.0|\n",
      "|              INTUIT INC.|      7.0|\n",
      "+-------------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df7 = df2.select(df2.EMPLOYER_NAME_CASE_STATUS, df2.CERTIFIED.cast(\"float\"))\n",
    "df7.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "share_udf = udf(share, FloatType())\n",
    "df8 = df7.select(\"EMPLOYER_NAME_CASE_STATUS\", share_udf(df7.CERTIFIED).alias(\"%share\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+------------+\n",
      "|EMPLOYER_NAME_CASE_STATUS|      %share|\n",
      "+-------------------------+------------+\n",
      "|     AMAZON CORPORATE LLC| 0.012051096|\n",
      "|     ADOBE SYSTEMS INC...| 0.007712702|\n",
      "|               APPLE INC.|0.0048204386|\n",
      "|      CISCO SYSTEMS, INC.| 0.003856351|\n",
      "|              GOOGLE INC.| 0.003615329|\n",
      "|     WAL-MART ASSOCIAT...| 0.003374307|\n",
      "|             VMWARE, INC.|0.0028922632|\n",
      "|          EMC CORPORATION|0.0026512414|\n",
      "|     BECTON, DICKINSON...|0.0021691974|\n",
      "|     SEARS HOLDINGS MA...|0.0021691974|\n",
      "|              DROGA5, LLC|0.0019281755|\n",
      "|               ABBVIE INC|0.0019281755|\n",
      "|     BURGER KING CORPO...|0.0019281755|\n",
      "|     MICROSOFT CORPORA...|0.0019281755|\n",
      "|          ECOLAB USA INC.|0.0019281755|\n",
      "|        EPAM SYSTEMS, INC|0.0019281755|\n",
      "|            NETFLIX, INC.|0.0019281755|\n",
      "|      GENENTECH USA, INC.|0.0019281755|\n",
      "|       CVS PHARMACY, INC.|0.0019281755|\n",
      "|      GENERAL MILLS, INC.|0.0016871535|\n",
      "+-------------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here’s a small gotcha** — because Spark UDF doesn’t convert integers to floats, unlike Python function which works for both integers and floats, a Spark UDF will return a column of NULLs if the input data type doesn’t match the output data type, as in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "share_integer_udf = udf(share, IntegerType())\n",
    "df9 = df7.select(\"EMPLOYER_NAME_CASE_STATUS\", share_integer_udf(df7.CERTIFIED).alias(\"%share\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+------+\n",
      "|EMPLOYER_NAME_CASE_STATUS|%share|\n",
      "+-------------------------+------+\n",
      "|     AMAZON CORPORATE LLC|  null|\n",
      "|     ADOBE SYSTEMS INC...|  null|\n",
      "|               APPLE INC.|  null|\n",
      "|      CISCO SYSTEMS, INC.|  null|\n",
      "|              GOOGLE INC.|  null|\n",
      "|     WAL-MART ASSOCIAT...|  null|\n",
      "|             VMWARE, INC.|  null|\n",
      "|          EMC CORPORATION|  null|\n",
      "|     BECTON, DICKINSON...|  null|\n",
      "|     SEARS HOLDINGS MA...|  null|\n",
      "|     MICROSOFT CORPORA...|  null|\n",
      "|      GENENTECH USA, INC.|  null|\n",
      "|       CVS PHARMACY, INC.|  null|\n",
      "|              DROGA5, LLC|  null|\n",
      "|          ECOLAB USA INC.|  null|\n",
      "|     BURGER KING CORPO...|  null|\n",
      "|        EPAM SYSTEMS, INC|  null|\n",
      "|            NETFLIX, INC.|  null|\n",
      "|               ABBVIE INC|  null|\n",
      "|      GENERAL MILLS, INC.|  null|\n",
      "+-------------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df9.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Spark Dataframe to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'EMPLOYER_NAME_CASE_STATUS', u'%share'], dtype='object')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df = df9.toPandas()\n",
    "pandas_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: SQL Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a temproary table view from a Dataframe, which can be further used to perform SQL queries on the data. In part 1, we saw operations using Dataframes. We will pick the same dataset and perform some basic SQL queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a table out of `df2`**\n",
    "\n",
    "Hint: We can use `registerTempTable(name)` method on any dataframe to create a table out of it.\n",
    "\n",
    "**`registerTempTable(name)`**: Registers this RDD as a temporary table using the given name.\n",
    "\n",
    "- The lifetime of this temporary table is tied to the SQLContext that was used to create this DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable(\"visa_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to perform SQL queries through Spark?\n",
    "\n",
    "TO perform SQL Queries we can `SparkSession.sql(sqlQuery)` where `sqlQuery` can be any valid sql query.\n",
    "\n",
    "- **`SparkSession.sql(sqlQuery)`**: **Returns a DataFrame** representing the result of the given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visa = spark.sql(\"select * from visa_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's check if the above query gave us the identical dataframe in the result!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(df_visa.collect()) == sorted(df.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|       EMPLOYER_NAME|CERTIFIED_COUNT|\n",
      "+--------------------+---------------+\n",
      "|AMAZON CORPORATE LLC|             50|\n",
      "|ADOBE SYSTEMS INC...|             32|\n",
      "|          APPLE INC.|             20|\n",
      "| CISCO SYSTEMS, INC.|             16|\n",
      "|         GOOGLE INC.|             15|\n",
      "|WAL-MART ASSOCIAT...|             14|\n",
      "|        VMWARE, INC.|             12|\n",
      "|     EMC CORPORATION|             11|\n",
      "|BECTON, DICKINSON...|              9|\n",
      "|SEARS HOLDINGS MA...|              9|\n",
      "+--------------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Top10 companies getting visa approval (for all the years)\n",
    "spark.sql(\"SELECT EMPLOYER_NAME, count(EMPLOYER_NAME) as CERTIFIED_COUNT FROM visa_table where CASE_STATUS = 'CERTIFIED' GROUP BY EMPLOYER_NAME order by CERTIFIED_COUNT desc\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|           JOB_TITLE|Approved|\n",
      "+--------------------+--------+\n",
      "|  OPERATIONS MANAGER|     206|\n",
      "|   MARKETING MANAGER|     191|\n",
      "|     GENERAL MANAGER|     128|\n",
      "|CHIEF EXECUTIVE O...|     127|\n",
      "|     PRODUCT MANAGER|      94|\n",
      "+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT JOB_TITLE, count(*) as Approved FROM visa_table where CASE_STATUS = 'CERTIFIED' GROUP BY JOB_TITLE order by Approved desc\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in our dataset the Job Title as `OPERATIONS MANAGER` has got highest number of approvals.\n",
    "\n",
    "**Let's find out the `EMPLOYER_NAME` having the highest number of operations manager getting visa approved.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|       EMPLOYER_NAME|Approved|\n",
      "+--------------------+--------+\n",
      "|     TAKETOURS, INC.|       3|\n",
      "|       SOLE COOL INC|       3|\n",
      "|PRINTRONIC CORPOR...|       2|\n",
      "|MARUTI MANAGEMENT...|       2|\n",
      "|JAMSAN HOTEL MANA...|       2|\n",
      "+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT EMPLOYER_NAME,count(*) as Approved FROM visa_table where CASE_STATUS = 'CERTIFIED' AND JOB_TITLE ='OPERATIONS MANAGER' GROUP BY EMPLOYER_NAME order by Approved desc\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next, find out the approved applications having the highest paid salaries.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+------------------+--------------------+----+------------------+-----------+\n",
      "|          businesses|   wage|          SOC_NAME|           JOB_TITLE|YEAR|FULL_TIME_POSITION|CASE_STATUS|\n",
      "+--------------------+-------+------------------+--------------------+----+------------------+-----------+\n",
      "|BANDY CANYON RANC...|99986.0|  CHIEF EXECUTIVES|CHIEF EXECUTIVE O...|2016|                 Y|  CERTIFIED|\n",
      "|CALIFORNIA GALVAN...|99986.0|  CHIEF EXECUTIVES|CHIEF EXECUTIVE O...|2016|                 Y|  CERTIFIED|\n",
      "|         LOMICS, LLC|99986.0|  CHIEF EXECUTIVES|                 CEO|2016|                 Y|  CERTIFIED|\n",
      "|UC UNIVERSITY HIG...|99986.0|  CHIEF EXECUTIVES|CHIEF FINANCIAL O...|2016|                 Y|  CERTIFIED|\n",
      "|         SUN BUM LLC|99986.0|  CHIEF EXECUTIVES|CHIEF EXECUTIVE O...|2016|                 Y|  CERTIFIED|\n",
      "|THE STUDENT LOAN ...|99972.0|MARKETING MANAGERS|     PRODUCT MANAGER|2016|                 Y|  CERTIFIED|\n",
      "|SEARS HOLDINGS MA...|99972.0|MARKETING MANAGERS|MANAGER, BUSINESS...|2016|                 Y|  CERTIFIED|\n",
      "|SEARS HOLDINGS MA...|99972.0|MARKETING MANAGERS|   DIRECTOR, PRICING|2016|                 Y|  CERTIFIED|\n",
      "|SEARS HOLDINGS MA...|99972.0|MARKETING MANAGERS|DIRECTOR, PROGRAM...|2016|                 Y|  CERTIFIED|\n",
      "|THE STUDENT LOAN ...|99972.0|MARKETING MANAGERS|     PRODUCT MANAGER|2016|                 Y|  CERTIFIED|\n",
      "+--------------------+-------+------------------+--------------------+----+------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT EMPLOYER_NAME as businesses, PREVAILING_WAGE as wage, SOC_NAME, JOB_TITLE, YEAR, FULL_TIME_POSITION, CASE_STATUS  FROM visa_table where CASE_STATUS ='CERTIFIED' order by PREVAILING_WAGE desc\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next, determine maximum salaries by `JOB_TITLE` for FULL TIME POSITIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|           JOB_TITLE|Max_Salary|\n",
      "+--------------------+----------+\n",
      "|CHIEF EXECUTIVE O...|   99986.0|\n",
      "|CHIEF EXECUTIVE O...|   99986.0|\n",
      "|CHIEF FINANCIAL O...|   99986.0|\n",
      "|                 CEO|   99986.0|\n",
      "|     PRODUCT MANAGER|   99972.0|\n",
      "|DIRECTOR, BUSINES...|   99972.0|\n",
      "|   DIRECTOR, PRICING|   99972.0|\n",
      "|DIRECTOR, PROGRAM...|   99972.0|\n",
      "|MANAGER, BUSINESS...|   99972.0|\n",
      "|PLASTICS MOVEMENT...|   99819.0|\n",
      "+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT JOB_TITLE ,MAX(PREVAILING_WAGE) as Max_Salary FROM visa_table where CASE_STATUS ='CERTIFIED' AND  FULL_TIME_POSITION ='Y' GROUP BY JOB_TITLE ORDER BY Max_Salary DESC\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
